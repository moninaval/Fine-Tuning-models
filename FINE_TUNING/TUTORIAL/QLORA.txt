How QLoRA Works
All weights inside the transformer block are quantized to 4-bit and frozen.
Examples:

Attention: q_proj, k_proj, v_proj, out_proj

Feedforward: w0, w1, fc1, fc2

ğŸ”’ These are:

Loaded in 4-bit (bnb_4bit)

Frozen

Not trainable directly

âœ… But you can inject LoRA adapters into some or all of these via target_modules.
These adapters are full-precision, trainable, and added during training.

â— You choose which of these quantized weights get LoRA adapters â€” thatâ€™s configurable.



Weights outside the transformer block are not quantized, and can be optionally trained.
Examples:

embed_tokens (input embedding layer)

lm_head (output layer)

LayerNorm modules

These are:

Left in full precision (e.g., FP16 or BF16)

Frozen by default

Can be unfrozen via config for training

âœ… If you choose to train these, their new weights will replace the originals after training (e.g., lm_head.weight = trained_lm_head.weight)


Transformer block:
  q_proj  = [4-bit frozen] + [LoRA adapter] âœ…
  v_proj  = [4-bit frozen] + [LoRA adapter] âœ…
  w1      = [4-bit frozen] + [LoRA adapter] âœ…

Outside block:
  lm_head = [trained full-precision] âœ… (if enabled)
  embed_tokens = [trained full-precision] âœ… (if enabled)
  norm_layers = [optional training] âœ…
