peft:
  mode: qlora
  task_type: "CAUSAL_LM"
  target_modules: ["q_proj", "v_proj", "w1"]     # w0, w1, fc1, fc2  are wights that can be included as per use case (All are transformer block weights)
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"

  trainable_parts:
    train_embeddings: false                   #outside transformer block
    train_output_head: true                   #outside transformer block
    train_norm_layers: false                  #outside transformer block

quantization:
  enabled: true
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
