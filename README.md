# ðŸ”§ LLM Fine-Tuning Framework (LoRA / QLoRA / Full FT)

This repository provides a flexible, modular framework to fine-tune transformer-based language models (like **Phi-2**, **Mistral**, **LLaMA**, etc.) using:
- âœ… **LoRA** (Low-Rank Adaptation)
- âœ… **QLoRA** (Quantized LoRA for low-VRAM GPUs)
- âœ… **Full fine-tuning** (for large-scale systems)

You can configure everything â€” from which model to use, to what parts of it to adapt/train â€” using YAML configuration files.# Fine-Tuning-Phi-3
This project supports fine-tuning large language models like Phi-2 or Phi-3 using tokenized datasets in HuggingFace format (.arrow). It supports 3 types of inputs:

ðŸ”¤ Raw .txt files
ðŸ“˜ Instruction-style .jsonl files
ðŸ’¬ Chat-style .jsonl files

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ incoming_text/          # Raw .txt files
â”‚   â”œâ”€â”€ incoming_instruction/   # Instruction .jsonl files
â”‚   â”œâ”€â”€ incoming_chat/          # Chat .jsonl files
â”‚   â””â”€â”€ tokenized/              # Output tokenized datasets (.arrow)
â”‚
â”œâ”€â”€ checkpoints/                # Model checkpoints (LoRA adapters + optimizer state)
â”œâ”€â”€ config/                     # YAML configuration files for training
â”œâ”€â”€ seen_datasets.json          # Tracks already tokenized files
â”œâ”€â”€ prepare_raw_text_dataset.py
â”œâ”€â”€ prepare_instruction_dataset.py
â”œâ”€â”€ prepare_chat_dataset.py
â”œâ”€â”€ train.py                    # QLoRA training entry point



 Dataset Preparation
1.	Tokenize from raw .txt:
python scripts/prepare_dataset.py --model_id microsoft/phi-3-mini-4k-instruct --input_dir data/incoming_instruction --output_dir data/tokenized/

2.	tokenize from instruction .jsonl:
python scripts/prepare_instruction_dataset.py --model_id microsoft/phi-3-mini-4k-instruct --input_dir data/incoming_instruction --output_dir data/tokenized/

3.	tokenize from chat .jsonl:
python scripts/prepare_chat_dataset.py --model_id microsoft/phi-3-mini-4k-instruct --input_dir data/incoming_instruction --output_dir data/tokenized/
4.	Fine-Tune the Model:
python scripts/train.py --train_config config/train_qlora.yaml --model_config config/model_phi3.yaml --experiment_config config/experiment.yaml --dataset_name alpaca

