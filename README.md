# ðŸ”§ LLM Fine-Tuning Framework (LoRA / QLoRA / Full FT)

This repository provides a flexible, modular framework to fine-tune transformer-based language models (like **Phi-2**, **Mistral**, **LLaMA**, etc.) using:
- âœ… **LoRA** (Low-Rank Adaptation)
- âœ… **QLoRA** (Quantized LoRA for low-VRAM GPUs)
- âœ… **Full fine-tuning** (for large-scale systems)

You can configure everything â€” from which model to use, to what parts of it to adapt/train â€” using YAML configuration files.# Fine-Tuning-Phi-3